"""
title: Conditional RAG Pipeline With Llama_Index And Ollama
author: Jun
author_url: https://github.com/Jundifang
date: 2025-05
version: 1.0
license: MIT
description: The pipeline of PsyLite for conditional retrieving relevant information from a knowledge base using the Llama Index library with Ollama embeddings.
requirements: llama-index, llama-index-llms-ollama, llama-index-embeddings-ollama
"""

from typing import List, Union, Generator, Iterator, Optional
from schemas import OpenAIChatMessage
import os
import re
from utils.pipelines.main import get_last_user_message, get_last_assistant_message,get_system_message
from openai import OpenAI 
import requests
from pydantic import BaseModel
import time
from pprint import pprint



class Pipeline:

    class Valves(BaseModel):
        LLAMAINDEX_OLLAMA_BASE_URL: str
        LLAMAINDEX_MODEL_NAME: str
        OLLAMA_MODEL_NAME: str
        LLAMAINDEX_EMBEDDING_MODEL_NAME: str
        JUDEGE_MODEL_NAME: str
        JUDGE_API_BASE_URL: str
        JUDGE_MODEL_API: str
        TAG_GEN_MODEL_NAME: str

    def __init__(self):
        self.documents = None
        self.index = None
        self.judge_result= None
        self.rag_result = None
        self.judge_code=None
        self.activate_model = None
        self.valves = self.Valves(
            **{
                "LLAMAINDEX_OLLAMA_BASE_URL": os.getenv("LLAMAINDEX_OLLAMA_BASE_URL", "http://host.docker.internal:11434"),
                "LLAMAINDEX_MODEL_NAME": os.getenv("LLAMAINDEX_MODEL_NAME", "qwen3:0.6b"),
                "LLAMAINDEX_EMBEDDING_MODEL_NAME": os.getenv("LLAMAINDEX_EMBEDDING_MODEL_NAME", "bge-m3:latest"),
                "JUDEGE_MODEL_NAME": os.getenv("JUDEGE_MODEL_NAME", "glm-4-flash-250414"),
                "JUDGE_API_BASE_URL": os.getenv("JUDGE_API_BASE_URL", ""),
                "JUDGE_MODEL_API": os.getenv("JUDGE_MODEL_API",""),
                "OLLAMA_MODEL_NAME": os.getenv("OLLAMA_MODEL_NAME", "qwen3:0.6b"),
                "TAG_GEN_MODEL_NAME":  os.getenv("TAG_GEN_MODEL_NAME", "qwen3:0.6b")
            }
        )

    async def inlet(self, body: dict, user: Optional[dict] = None) -> dict:
        # This function is called before the OpenAI API request is made. You can modify the form data before it is sent to the OpenAI API.
        
        print(f"inlet: {__name__} - body:")
        pprint(body)

        user_message = get_last_user_message(body["messages"])
        
        
        pprint(f"user_messages:{user_message}")
        messages=body["messages"][:-1]
        pprint(f"messages_list:{messages}")
        
        if "task" in body["metadata"]:
            print("titie / tag generated")
            self.activate_model=self.valves.TAG_GEN_MODEL_NAME
            self.judge_result = None
        else:
            print("user generated") 
            self.activate_model=self.valves.OLLAMA_MODEL_NAME
            self.judge_result = self.request(user_message,messages)
            
        print(2*"\n", "Raw judge_result:", self.judge_result, 2*"\n")

        return body
    
    async def outlet(self, body: dict, user: Optional[dict] = None) -> dict:
        print(f"outlet:{__name__}")
        messages = body["messages"]

        assistant_message = get_last_assistant_message(messages)
        print(f"Assistant message: {assistant_message}")
        
        if self.judge_code == "2":
            # å¤„ç† self.rag_resultï¼šåªä¿ç•™ä»â€œç”²ï¼šâ€å¼€å§‹çš„å†…å®¹
            if self.rag_result:
                match = re.search(r'ç”²ï¼š', self.rag_result)
                if match:
                    rag_content = self.rag_result[match.start():]  # ä»â€œç”²ï¼šâ€å¼€å§‹æˆªå–
                else:
                    rag_content = self.rag_result  # æ²¡æ‰¾åˆ°â€œç”²ï¼šâ€ï¼Œä¿ç•™åŸæ ·
            else:
                rag_content = ""
                
            # å¤„ç† self.rag_resultï¼šæ›¿æ¢æ‰€æœ‰ \n\n ä¸º \n
            rag_content = rag_content.replace("\n\n", "\n") if rag_content else ""
            
            print(f"final rag_content: {rag_content}")
            
            for message in reversed(messages):
                if message["role"] == "assistant":
                    message["content"] = assistant_message+f"\n\n<details>\n<summary>å¾ˆé«˜å…´çœ‹åˆ°ä½ å¿ƒæƒ…æ„‰æ‚¦çš„æ ·å­ï¼Œè¿™ä¸€æ®µç›¸å£°é€ç»™ä½ ï¼Œå¸Œæœ›ä½ å–œæ¬¢~</summary>\n```ç›¸å£°\n{rag_content}\n```\n\n</details>"
                    break

        return body
    
    
    
    
    def request(self,user_message: str,messages: List[dict]):

        print("æ‰§è¡ŒçŠ¶æ€è¯„ä¼°ing")
        client = OpenAI(
            api_key=self.valves.JUDGE_MODEL_API,
            base_url=self.valves.JUDGE_API_BASE_URL
        ) 
        system_prompt = '''
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¿ƒç†å¥åº·è¯„ä¼°å¸ˆ,ä½ å¿…é¡»ç»¼åˆè€ƒé‡ç”¨æˆ·çš„å›å¤ä»¥åŠå†å²èŠå¤©è®°å½•ï¼Œç»™ç”¨æˆ·çš„å½“å‰çŠ¶å†µè¿›è¡Œæ‰“åˆ†
        
        è¾“å‡ºè¦æ±‚ï¼š{0æˆ–1æˆ–2},{æ‰“åˆ†ç†ç”±}
        æ‰“åˆ†è¯´æ˜ï¼š0ä»£è¡¨ç”¨æˆ·å½“å‰æƒ…å†µéå¸¸å±é™©ï¼Œå¿…é¡»é©¬ä¸Šåœæ­¢AIå¿ƒç†è¾…å¯¼ï¼Œè½¬ç”±äººå·¥è¿›è¡Œå¿ƒç†å’¨è¯¢ã€å¿ƒç†æ²»ç–—ã€‚
        1ä»£è¡¨ç”¨æˆ·å½“å‰æƒ…å†µæ­£å¸¸ï¼Œå¯ä»¥ç»§ç»­ä¸AIè¿›è¡Œå¯¹è¯
        2ä»£è¡¨ç”¨æˆ·å½“å‰å¿ƒæƒ…æ„‰æ‚¦ï¼Œå¯ä»¥åœ¨å¯¹è¯ä¸­å¢åŠ å¹½é»˜ã€è°ƒä¾ƒçš„è¯­å¥,æ¯”å¦‚ç›¸å£°ã€è„±å£ç§€çš„æ–¹å¼æ¥æå‡ç”¨æˆ·ä½“éªŒã€‚
        '''
        
        message_content=f'''
        ç”¨æˆ·å›å¤ï¼š{user_message}
        å†å²èŠå¤©è®°å½•ï¼š{messages}
        '''
        
        completion = client.chat.completions.create(
            model=self.valves.JUDEGE_MODEL_NAME,
            messages=[    
                {"role": "system", "content": system_prompt},    
                {"role": "user", "content": message_content},
            ],
            top_p=0.7,
            temperature=0
        ) 
        
        return completion.choices[0].message.content
        
    async def on_startup(self):
        from llama_index.embeddings.ollama import OllamaEmbedding
        from llama_index.llms.ollama import Ollama
        from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader

        Settings.embed_model = OllamaEmbedding(
            model_name=self.valves.LLAMAINDEX_EMBEDDING_MODEL_NAME,
            base_url=self.valves.LLAMAINDEX_OLLAMA_BASE_URL,
        )
        Settings.llm = Ollama(
            model=self.valves.LLAMAINDEX_MODEL_NAME,
            base_url=self.valves.LLAMAINDEX_OLLAMA_BASE_URL,
        )

        # This function is called when the server is started.
        global documents, index

        self.documents = SimpleDirectoryReader("/app/data").load_data()
        if self.documents is None:
            print( "No documents found")
        self.index = VectorStoreIndex.from_documents(self.documents)

        pass

    async def on_shutdown(self):
        # This function is called when the server is stopped.
        pass

    def pipe(
    self, user_message: str, model_id: str, messages: List[dict], body: dict
) -> Union[str, Generator, Iterator]:
    

        if self.index is None:
            return "No index found"

        # è§£æ judge_result ä¸­çš„åºå·éƒ¨åˆ†
        if self.judge_result is not None:
            try:
                match = re.search(r'\d', self.judge_result)
                if match:
                    self.judge_code = match.group(0)
                else:
                    raise ValueError("No digit found in judge_result")
            except Exception as e:
                print(f"Failed to parse judge_result: {e}")
                self.judge_code = ""

            print("Parsed judge_code:", self.judge_code)

        # 0ï¼šéå¸¸å±é™©ï¼Œæ¨èäººå·¥ä»‹å…¥
        if self.judge_code == "0":
            print("çŠ¶æ€è¯„ä¼°ç»“æœï¼š0")
            return "å¾ˆæŠ±æ­‰ï¼Œæˆ‘åªæ˜¯ä¸€ä¸ªAIå°åŠ©æ‰‹ï¼Œæ‚¨çš„æƒ…å†µæˆ‘åŠ›ä¸ä»å¿ƒï¼Œè¯·æ‚¨æ‹¨æ‰“å…¨å›½å„åœ°å…è´¹å¿ƒç†çƒ­çº¿12356ï¼Œæˆ–è€…å¯»æ±‚ä¸“ä¸šå¿ƒç†å’¨è¯¢å¸ˆçš„å¸®åŠ©ï¼Œè¯·æ‚¨ç†è§£ğŸŒ¹"

        # 2ï¼šç”¨æˆ·å¿ƒæƒ…å¥½ï¼Œå¯ä»¥åŠ å…¥å¹½é»˜å…ƒç´ 
        else:
            if self.judge_code == "2":
                print("çŠ¶æ€è¯„ä¼°ç»“æœï¼š2")

                system_prompt = get_system_message(messages)

                prompt = f'''{system_prompt}

                    åŒæ—¶ä½ ä¹Ÿæ˜¯ä¸€ä¸ªéå¸¸æŠŠæ¡å°ºåº¦åˆ†å¯¸çš„ç›¸å£°å¤§å¸ˆï¼Œå½“å‰æƒ…å†µæ˜¯ï¼šç”¨æˆ·å½“å‰æƒ…å†µå¥½ï¼Œé€‚åˆä½¿ç”¨ç›¸å£°ã€è„±å£ç§€çš„æ–¹å¼æ¥ç¼“è§£æ°›å›´ã€æ‹‰è¿›è·ç¦»ï¼Œä½ éœ€è¦ä¸€æ­¥æ­¥æ·±åº¦æ€è€ƒä¸€ä¸ªç›¸å£°å°æ®µå­ï¼Œä½ å¯ä»¥åœ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢é€‚åˆå½“å‰è¯­å¢ƒçš„ç›¸å£°ç‰‡æ®µï¼Œæˆ–è€…è‡ªå·±å†™ä¸€æ®µç¬¦åˆè¯­å¢ƒçš„ã€å®Œæ•´ç®€å•çš„å¯¹è¯ç›¸å£°æ®µå­
                    è¾“å‡ºè¦æ±‚:
                    1. ç›¸å£°æ®µå­5-10è¡Œå³å¯
                    2. ä½¿ç”¨ç›¸å£°æ®µå­è®²æ±‚å¾ªåºæ¸è¿›ï¼Œå…·æœ‰æ•™è‚²æ„ä¹‰
                    3. ä¸å…è®¸æ·»åŠ ä»»ä½•æè¿°æ€§æ–‡å­—æˆ–è¯´æ˜ï¼Œæ¯æ®µå¯¹è¯å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ ¼å¼ï¼š
                        ç”²ï¼šå°è¯
                        ä¹™ï¼šå°è¯
                        

                    å·²çŸ¥ç”¨æˆ·å›å¤ï¼š{user_message}
                /no_think'''

                query_engine = self.index.as_query_engine()
                self.rag_result  = query_engine.query(prompt).response
                print(f"æ£€ç´¢ç»“æœï¼š{self.rag_result}")
                # body["messages"][-1]["content"] = f"ä½ éœ€è¦å›åº”ç”¨æˆ·çš„é—®é¢˜ï¼Œå¹¶åœ¨å›å¤ä¸­çš„æ°å½“ä½ç½®åŸå°ä¸åŠ¨åœ°æ’å…¥ç›¸å£°æ®µå­ã€‚\n\nç”¨æˆ·å›å¤ï¼š{user_message}"

            # é»˜è®¤æƒ…å†µï¼š1 æˆ–æ— æ³•è¯†åˆ«ï¼Œæ­£å¸¸æµç¨‹
            else:
                print(f"æœªçŸ¥æˆ–é»˜è®¤å¤„ç† (judge_code={self.judge_code})ï¼Œç»§ç»­å¸¸è§„å¯¹è¯")

            # è°ƒç”¨LLMç”Ÿæˆæœ€ç»ˆå›å¤
            try:
                print("ç”Ÿæˆæœ€ç»ˆç»“æœ")

                r = requests.post(
                    url=f"{self.valves.LLAMAINDEX_OLLAMA_BASE_URL}/v1/chat/completions",
                    json={**body, "model": self.activate_model},
                    stream=True,
                )

                r.raise_for_status()

                if body["stream"]:
                    return r.iter_lines()
                else:
                    return r.json()
            except Exception as e:
                return f"Error: {e}"
            
        
